\section{Testing plan}

The main goals of this performance testing plan are:
\begin{itemize}
  \item to find out whether the usage of LSM implies some sort of performance penalty;
  \item to identify possible improvements when embedding WebAssembly in another language.
\end{itemize}

\noindent
For a given program written in Rust, the following configurations are tested in order to compare
overall speed of both restricted and unrestricted scenarios:
\begin{itemize}
  \item a native binary, compiled by \texttt{rustc} and run:
        \begin{itemize}
          \item without a sandbox;
          \item sandboxed with Landlock;
          \item sandboxed with eBPF.
        \end{itemize}
  \item a WebAssembly binary obtained by the same Rust program and run:
        \begin{itemize}
          \item directly by the \textit{Wasmtime} command line tool;
          \item by the WASM runtime developed in Section \ref{sec:restricting-wasi-landlock} and restricted with Landlock;
          \item by the WASM runtime developed in Section \ref{sec:restricting-wasi-landlock} and restricted with
                eBPF\footnote{In this case, Landlock is disabled.}.
        \end{itemize}
\end{itemize}

When testing the native binary with Landlock, the program has to be modified. That is,
some instructions are added to the code that setup Landlock in a way compatible with the
specific test. An example of the required instructions is visible in Listing \ref{lst:test-program-landlock-example}.
On the other hand, for the other methods (i.e., without a sandbox and with eBPF) the program
does not need any change, but in the case of eBPF it is necessary to use the \texttt{bpfcontain run}
command with the name of the binary to actually apply the sandbox.

\vspace*{0.5cm}
\begin{code}[language=Rust, caption=An example of a program restricted with Landlock., label=lst:test-program-landlock-example]
use anyhow::Result;
use landlock::*;

fn main() -> Result<()> {
    let path_fd = PathFd::new("input-file.txt")?;
    let access = make_bitflags!(AccessFs::{ ReadFile });
    
    let path_beneath = PathBeneath::new(path_fd)
      .allow_access(access);
    let _ = Ruleset::new()
        .handle_access(AccessFs::from_all(ABI::V1))?
        .create()?
        .add_rule(path_beneath)?
        .restrict_self()?;

    // Here goes the main code ...

    Ok(())
}
\end{code}

Moreover, the internal performance of the WASM runtime developed in Section \ref{sec:restricting-wasi-landlock}
and the performance of the sandboxing methods will be tested in Section \ref{sec:performance-internal-analysis}.

\subsection{System and hardware}

The system and hardware used for both the development and test of the project described in Section \ref{sec:restricting-wasi-landlock}
is as follows:
\begin{itemize}
  \item Arch Linux \cite{arch-linux} as the operating system, more specifically the 2022.05.01 version;
  \item an Intel Core i5-7200U quad-core with a clock rate of 2.5 GHz;
  \item 8 GB of RAM;
  \item a 120 GB solid state disk.
\end{itemize}

The choice of the operating system is mainly dictated by the fact that Arch Linux has both Landlock and eBPF active
out of the box, removing the need to compile the Linux kernel with the necessary flags to enable these
functionalities.

\subsection{Test scenarios}
\label{sec:performance-test-description}

The performed tests are:
\begin{itemize}
  \item a purely computational program, given by the sorting of 10000 random numbers as in Listing \ref{lst:sorting-test-rust},
        in order to measure only the computational impact of the various methods;
  \item a simple reading of files of various sizes as in Listing \ref{lst:reading-test-rust}, with only the necessary permissions enabled on
        a case-by-case base, in order to test the sandbox provided and how they fare against native binaries.
\end{itemize}

The reading file test is repeated with different file sizes\footnote{Randomly generated by using \texttt{/dev/urandom}.},
which are $100$ KB, $1$ MB, $10$ MB and $100$ MB. By doing this, it is possible to see how performance
varies when dealing with progressively larger files. Moreover, an additional empty file is used to
test the overhead on the pure opening of a file.

\vspace*{0.5cm}
\begin{code}[language=Rust, caption=The ``sorting program''., label=lst:sorting-test-rust]
use rand::Rng;

fn main() {
  let mut rng = rand::thread_rng();
  let mut vec: Vec<i32> = Vec::new();

  for _ in 0..10000 {
    vec.push(rng.get::<i32>());
  }

  vec.sort();
}
\end{code}

\begin{code}[language=Rust, caption=The ``reading program''., label=lst:reading-test-rust]
fn main() {
  let content = std::fs::read("input-file.txt");
  match content {
      Ok(c) => println!("{}", c.len()),
      Err(_e) => std::process::exit(1),
  }
}
\end{code}

\subsection{Performance indicators}

The main performance indicators will be the mean execution time, measured in milliseconds, together with its
standard deviation in order to compensate for variability.
These measures are always obtained from a sample of 100 runs, executed and measured by \texttt{hyperfine},
a command-line benchmarking tools \cite{hyperfine}, and finally saved to a
JSON file.

In this chapter, we define for brevity $\mu_x$ to be the mean of the method $x$ and $\sigma_x$ to be
the standard deviation of the method $x$.

\section{Comparison between different sandboxes}
\label{sec:performance-comparison-between-different-sandboxes}

In this section we will look at what impact different LSMs bring on a program,
be it a native binary or a WebAssembly binary run through some kind of runtime.

In Table \ref{table:lsm-impact-native} are reported all execution times for a native binary,
compiled directly with \texttt{rustc} and run both unrestricted (\textit{Native}) and restricted
with different LSMs (\textit{Landlock} and \textit{eBPF}).

On the other hand, in Table \ref{table:lsm-impact-wasm} are listed the runs of a WebAssembly binary through
\textit{Wasmtime} without any restriction, or through the WASM runtime developed in Section \ref{sec:restricting-wasi-landlock}
and restricted with Landlock (\textit{WL}) or eBPF (\textit{WeBPF}).

\begin{table}
  \centering
  \csvreader[
    tabular={|l|r|r|r|r|r|r|},
    table head = \hline
      \textit{Test type}
      & $\mu_\mathrm{Native}$   & $\sigma_\mathrm{Native}$
      & $\mu_\mathrm{Landlock}$ & $\sigma_\mathrm{Landlock}$
      & $\mu_\mathrm{eBPF}$     & $\sigma_\mathrm{eBPF}$ \\ \hline\hline,
    late after line = \\ \hline,
    head to column names,
  ]{test-data/native-landlock-ebpf.csv}{}
  {\type & \mnative & \snative & \mlandlock & \slandlock & \mebpf & \sebpf}
  \caption{Execution times of a native binary under different restrictions (in $ms$).}
  \label{table:lsm-impact-native}
\end{table}

\begin{table}
  \centering
  \csvreader[
    tabular={|l|r|r|r|r|r|r|},
    table head = \hline
      \textit{Test type}
      & $\mu_\mathrm{Wasmtime}$   & $\sigma_\mathrm{Wasmtime}$
      & $\mu_\mathrm{WL}$ & $\sigma_\mathrm{WL}$
      & $\mu_\mathrm{WeBPF}$     & $\sigma_\mathrm{WeBPF}$ \\ \hline\hline,
    late after line = \\ \hline,
    head to column names,
  ]{test-data/wasm-landlock-ebpf.csv}{}
  {\type & \mnative & \snative & \mlandlock & \slandlock & \mebpf & \sebpf}
  \caption{Execution times of a WASM binary under different restrictions (in $ms$).}
  \label{table:lsm-impact-wasm}
\end{table}

\subsection{A computational-heavy program}

Let's take a look at how restricting a purely computational-heavy program, namely the sorting of 
10000 random numbers shown in Listing \ref{lst:sorting-test-rust}, affects its performance.
In order to test the eBPF sandbox, Listing \ref{lst:outline-policy-sorting-test} shows the outline for the used policy.

Comparing the first lines of Table \ref{table:lsm-impact-native} and Table \ref{table:lsm-impact-wasm},
it is immediately apparent that using a LSM to restrict a binary, be it native or WebAssembly, worsen performance.

When dealing with a native binary, \textit{Landlock} has a lower overhead than \textit{eBPF} (an average
of $1.03\ ms$ for Landlock against an average of $2.30\ ms$ for eBPF), as is visible in Figure \ref{fig:distribution-sorting-native}.
This could be due to how the two sandboxes are enforced:
\begin{itemize}
  \item Landlock is directly available as a C library in the Linux kernel, so the communication between
        the program and the relative LSM can be as simple as multiple function calls;
  \item on the other hand, eBPF needs to have an active server\footnote{As highlighted in Section \ref{sec:restricting-wasi-ebpf}.},
        so the communication has to undergo a message exchange between processes.
\end{itemize}
By following this line of reasoning, it is expected that Landlock should be a little faster than eBPF.
Moreover, eBPF is more complex than Landlock since it allows more control on what
can and cannot be done by a running process, hence it has to deal with a higher complexity.
The overhead of eBPF, however, becomes smaller and smaller when measured in percentage as the execution times get higher.
This means that for small and fast programs the slow-down could become apparent if they get called many times
in a short period of time, while for slower program called with less frequency this should be negligible.

\vspace*{0.5cm}
\begin{code}[language=yaml, caption=The outline of the policy used for testing the sorting program., label=lst:outline-policy-sorting-test]
name: test-sorting
cmd: # The command to be executed
allow:
  - fs: {pathname: ., access: rxm}
\end{code}

When dealing with a WebAssembly binary, the computational-heavy program is noticeably slower
compared to the native binary execution times, either unrestricted or restricted, as shown in Figure
\ref{fig:distribution-sorting-wasm}\footnote{Note that Figures \ref{fig:distribution-sorting-native} and \ref{fig:distribution-sorting-wasm}
have different scales.}.
The same reasoning applied before is valid here too -- eBPF is a little slower than Landlock, with a overhead
of about $1\ ms$.
However, restricting a WASM binary with the runtime developed in Section \ref{sec:restricting-wasi-landlock}
introduces a constant overhead of around $30\ ms$ with respect to \textit{Wasmtime}.
Since it is shown in Table \ref{table:lsm-impact-native} that Landlock is quite close to
unrestricted performance on average, and because Landlock was disabled when running
the tests combining WASM and eBPF, this extra time could be due to how the WASM module gets
interpreted by the library provided by \textit{Wasmtime}\footnote{This will be analysed more
accurately in Section \ref{sec:performance-internal-analysis}.}.

\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{tests/box-plot-sorting.native.png}
    \caption{Native}
    \label{fig:distribution-sorting-native}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\linewidth}
    \centering
    \includegraphics[width=\textwidth]{tests/box-plot-sorting.wasm.png}
    \caption{WebAssembly}
    \label{fig:distribution-sorting-wasm}
  \end{subfigure}
  \caption{Execution times when sorting 10000 numbers.}
\end{figure}

\clearpage

\subsection{Opening a file}

This set of tests is meant to measure how the sandboxes perform when the restricted program opens a file.
In this case, the performance is measured by reading an empty file with the program described in Listing \ref{lst:reading-test-rust}.
For the policy used with eBPF, its outline is highlighted in Listing \ref{lst:outline-policy-reading-test}
-- in this case, it is necessary to give permissions to access the terminal since the size of the read content is printed. 

As Tables \ref{table:lsm-impact-native} and \ref{table:lsm-impact-wasm} show,
the same considerations made for a purely computational program are valid in this case too
-- Landlock, on average, is much closer to native speed than eBPF.
This is also shown in Figure \ref{fig:distribution-opening-native}, where the eBPF average is shifted towards
higher execution times.

Similarly, the WASM tests present the same behaviour remarked in the previous section --
even when unrestricted, WASM is much slower than a native binary, and the developed runtime
still has the constant overhead of around $30\ ms$ in both the tests within the Landlock and eBPF sandboxes.
Again, Figure \ref{fig:distribution-opening-wasm} shows the box plot of execution times.

\vspace*{1cm}

\begin{code}[language=yaml, caption=The outline of the policy used for testing the reading program., label=lst:outline-policy-reading-test]
name: test-reading-file

cmd: # The command to be executed

allow:
  - device: terminal
  - fs: {pathname: ., access: rxm}
\end{code}

\clearpage
\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{0.49\linewidth}
    \centering
    \includegraphics[width=\textwidth]{tests/box-plot-0.native.png}
    \caption{Native}
    \label{fig:distribution-opening-native}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\linewidth}
    \centering
    \includegraphics[width=\textwidth]{tests/box-plot-0.wasm.png}
    \caption{WebAssembly}
    \label{fig:distribution-opening-wasm}
  \end{subfigure}

  \caption{Execution times when opening a file.}
\end{figure}

\subsection{Accessing the file system}

This last batch of tests is meant to show how sandboxed binaries, both native and WASM, perform when
dealing with progressively larger files.
The following figures, namely Figure \ref{fig:avg-comparison-native-speed} and Figure \ref{fig:avg-comparison-wasm-speed},
represent how the average execution time of a native binary and a WASM binary, respectively, changes
when the read file gets bigger.

The first notable feature is that performance drops as the file gets larger,
starting to get worse when reading a file in the order of tens of MBs, and with a significant
spike in execution times when reaching the order of hundreds of MBs.
This slow-down is clearly visible in the native curves as well as in the WASM curves,
hence it has to be independent of the method used to run the
program and only due to external circumstances, such as the operating system or the disk reading speed.
For small files, however, there is not a notable change in speed.

Another point of note is that in Figure \ref{fig:avg-comparison-native-speed} the gap between eBPF and Landlock,
identified in the previous sections, is clearly visible at all file sizes.
This can be seen also in Figure \ref{fig:avg-comparison-wasm-speed}, albeit less clearly as the file reaches $100$ MB.
Note that, while the WASM runtime used for restricting a WASM binary with Landlock is the same used when testing eBPF,
in this last case Landlock was disabled altogether, so any performance difference is only due to eBPF's internals.

Again, the WASM curves in Figure \ref{fig:avg-comparison-wasm-speed} clearly show the overhead
of about $30\ ms$ identified in the previous sections -- the \textit{WASM+Landlock} and \textit{WASM+eBPF} curves
exhibit the same trend as the \textit{Wasmtime} one, but appear translated by a constant factor.

Finally, Figures \ref{fig:distribution-reading-100k}, \ref{fig:distribution-reading-1M}, \ref{fig:distribution-reading-10M},
and \ref{fig:distribution-reading-100M} collects all box plots relative to the different readings for
native binaries and WASM binaries.

\begin{figure}[ht!]
  \centering

  \begin{subfigure}[b]{0.49\linewidth}
    \centering
    \includegraphics[width=\textwidth]{tests/reading-file-native-graph.png}
    \caption{Native}
    \label{fig:avg-comparison-native-speed}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\linewidth}
    \centering
    \includegraphics[width=\textwidth]{tests/reading-file-wasm-graph.png}
    \caption{WebAssembly}
    \label{fig:avg-comparison-wasm-speed}
  \end{subfigure}

  \caption{A comparison of average speeds when reading a file.}
\end{figure}

\begin{figure}[ht!]
  \centering

  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{tests/box-plot-100K.native.png}
    \caption{Native}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{tests/box-plot-100K.wasm.png}
    \caption{WASM}
  \end{subfigure}

  \caption{Execution times when reading a 100 KB file.}
  \label{fig:distribution-reading-100k}
\end{figure}

\begin{figure}[ht!]
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{tests/box-plot-1M.native.png}
    \caption{Native}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{tests/box-plot-1M.wasm.png}
    \caption{WASM}
  \end{subfigure}

  \caption{Execution times when reading a 1 MB file.}
  \label{fig:distribution-reading-1M}
\end{figure}

\begin{figure}[ht!]
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{tests/box-plot-10M.native.png}
    \caption{Native}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{tests/box-plot-10M.wasm.png}
    \caption{WASM}
  \end{subfigure}

  \caption{Execution times when reading a 10 MB file.}
  \label{fig:distribution-reading-10M}
\end{figure}

\begin{figure}[ht!]
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{tests/box-plot-100M.native.png}
    \caption{Native}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{tests/box-plot-100M.wasm.png}
    \caption{WASM}
  \end{subfigure}

  \caption{Execution times when reading a 100 MB file.}
  \label{fig:distribution-reading-100M}
\end{figure}

\clearpage

\section{Internal analysis of the various sandboxes}
\label{sec:performance-internal-analysis}

The goal of this series of tests is to measure how the developed runtime,
described in Section \ref{sec:restricting-wasi-landlock}, behaves internally
as well as how Landlock and eBPF scale with the complexity of the rules
to implement.

The various measures are taken by using the standard Rust library \texttt{std::time},
in particular the \texttt{Instant} struct, and then averaged on a total of 100 runs.
The programs used as reference are:
\begin{itemize}
  \item a trivial one described in Listing \ref{lst:project-perf-program-simple},
        dealing only with printing a simple string to the terminal;
  \item a more complex one shown in Listing \ref{lst:project-perf-program-complex},
        reading and writing some files, so that specific permissions are required
        (specifically, reading and writing files).
\end{itemize}

\vspace*{0.5cm}

\begin{code}[language=Rust, caption=The trivial program., label=lst:project-perf-program-simple]
fn main() {
  println!("Hello World");
}
\end{code}

\begin{code}[language=Rust, caption=The more complex program., label=lst:project-perf-program-complex]
use std::fs::{read_to_string, write};

fn main() -> {
  read_to_string("file1.txt").expect("Read error");
  write("file1.txt", "Content 1").expect("Write error");
  read_to_string("file2.txt").expect("Read error");
  write("file2.txt", "Content 2").expect("Write error");
}  
\end{code}

\clearpage
\subsection{Execution times of the developed runtime}
\label{sec:performance-internal-analysis-execution-times}

In order to measure the execution times of the developed runtime,
various timestamps were taken and combined in specific parts of the
code in order to have some data on the execution time of important program parts,
which are the following:
\begin{itemize}
  \item command line argument parsing;
  \item WASM module instantiation;
  \item the preopening of all directories by the Wasmtime library;
  \item collecting and applying all the Landlock rules;
  \item and finally, compiling and running the WASM module.
\end{itemize}

Note that in this case only Landlock is considered, since eBPF is enforced externally
by a different process, while Landlock system calls are embedded in the code of the runtime.
Hence, only the impact of Landlock can be measured \textit{internally} in the code.

\vspace*{0.4cm}
\begin{table}[ht!]
  \centering
  \csvreader[
    tabular={|l|r|r|r|r|},
    table head = \hline
      \textit{Program section} & $\mu_I$ & $\sigma_I$ & $\mu_C$ & $\sigma_C$   \\ \hline\hline,
    late after line = \\ \hline,
    head to column names,
  ]{test-data/project-stats-simple.csv}{}
  {\type & \mean & \stddev & \meanc & \stddevc}
  \caption{Execution times in $ms$ when running the simple program (Listing \ref{lst:project-perf-program-simple}).}
  \label{table:execution-times-simple}
\end{table}

\begin{table}[ht!]
  \centering
  \csvreader[
    tabular={|l|r|r|r|r|},
    table head = \hline
      \textit{Program section} & $\mu_I$ & $\sigma_I$ & $\mu_C$ & $\sigma_C$   \\ \hline\hline,
    late after line = \\ \hline,
    head to column names,
  ]{test-data/project-stats-medium.csv}{}
  {\type & \mean & \stddev & \meanc & \stddevc}
  \caption{Execution times in $ms$ when running the complex program (Listing \ref{lst:project-perf-program-complex}).}
  \label{table:execution-times-complex}
\end{table}

Tables \ref{table:execution-times-simple} and \ref{table:execution-times-complex} contains
all measured data in milliseconds for, respectively, the trivial and complex program.
The recorded timestamps are both the average time spent by a section, given by $\mu_I$,
and the cumulative time up until that section's completion, given by $\mu_C$.
Standard deviations $\sigma_I$ and $\sigma_C$ are also given for any respective program part.
For example, if the part ``Landlock enforcement'' is such that $\mu_I = 20\ ms$ and $\mu_C = 100\ ms$,
this means that on average it takes $20\ ms$ to apply Landlock to the running process, and on average this part
is fully completed after $100\ ms$ from the beginning of the program.

Moreover, Figure \ref{fig:perf-execution-times-comparison} shows how each program section
occupies the overall time spent running by the program. The various labels are as follows
-- \texttt{A} is for ``Argument parsing'', \texttt{B} is for ``Module initialisation'',
\texttt{C} is for ``Preopen'', \texttt{D} is for ``Landlock enforcement'' and
\texttt{E} is for ``Running WASM binary''.

\begin{figure}[ht!]
  \centering
  
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{project-performance/graph_simple.png}
    \caption{Simple program.}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{project-performance/graph_simple_c.png}
    \caption{Simple program (cumulative).}
  \end{subfigure}

  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{project-performance/graph_medium.png}
    \caption{Complex program.}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{project-performance/graph_medium_c.png}
    \caption{Complex program (cumulative).}
  \end{subfigure}

  \caption{A graphic comparison of all the execution times.}
  \label{fig:perf-execution-times-comparison}
\end{figure}

As is immediately noticeable by the collected data, the biggest bottleneck on performance is given
by \textit{running the WebAssembly binary} with the library provided by \textit{Wasmtime}.
This section comprises of instantiating the required data structures, interpreting and linking the WASM module,
and finally running it, as already highlighted in Section \ref{sec:landlock-wasm-module}.
Running the WebAssembly binary alone makes up $97.7 \%$ of the time spent running the trivial program, and $98.1 \%$ of the time
spent running the complex program.

The next biggest chunk of time is spent into \textit{module initialisation}, which is comprised of the
reading of the WASM binary file into memory and the instantiation of specific context structures used
to keep track of preopened directories. For both programs, this is around $800\ \mu s$ in duration.

On the other hand, enforcing Landlock does not have a substantial impact on performance,
and it is even faster than the argument parsing algorithm.

This findings further support the data shown in Section \ref{sec:performance-comparison-between-different-sandboxes} --
it was found that the developed WASM runtime was slower than calling \textit{Wasmtime} from the command
line, but because Landlock had little overhead when dealing with native binaries this implied that
the ``culprit'' for the overhead could be the \textit{Wasmtime} library.
Moreover, in this case it can also be seen than the time took for running the WASM binary is, in both cases, about
the same as the constant overhead.

\clearpage
\subsection{Landlock performance scaling}
\label{sec:performance-internal-analysis-landlock-impact}

In this section we will focus on how Landlock impacts performance and how it scales
against the complexity of the specified rules. More specifically, performance is measured against
two ``modes'' of complexity scaling -- adding permissions to a single folder, and allowing access to multiple folders
with the same permissions.
In order to do this, the code of the developed WASM runtime will be used, and
the program tested will be the one described in Listing \ref{lst:project-perf-program-complex} -- Landlock enforcement
happens \textit{before} running the WASM program of choice, so how much time the restriction takes should not
depend on the specific program that has to be sandboxed.

\begin{figure}[hbt!]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{tests/landlock-impact.png}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{tests/landlock-impact-box.png}
  \end{subfigure}

  \caption{Relation between Landlock enforcement and number of permissions.}
  \label{fig:perf-test-landlock-impact-permissions}
\end{figure}

For the first series of tests, specific permissions will be added one at a time to a single folder,
starting from \texttt{R,W} (i.e., reading and writing files) and up to enabling all the permissions listed
in Table \ref{table:landlock-flags}.
Figure \ref{fig:perf-test-landlock-impact-permissions} shows how the average time varies against the number of permissions.
As is immediately visible, execution time does not depend on the number of permissions on a single folder --
the average time is always between $17\ \mu s$ and $19\ \mu s$, without great variance.
There are some outliers, but it is safe to conclude that the number of permissions does not affect performance
when it comes to enforcing Landlock on the running WASM runtime.
This is to be expected -- when enforcing permissions on a single folder, the code treats all permissions as numbers,
and when enforcing a rule on a single folder, all permissions are \textit{xor}-ed together.
This means that the cost of applying one rule is the same whatever the number of allowed actions is.

\begin{figure}[hbt!]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{tests/landlock-impact-folders.png}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{tests/landlock-impact-folders-box.png}
  \end{subfigure}

  \caption{Relation between Landlock enforcement and number of rules.}
  \label{fig:perf-test-landlock-impact-folders}
\end{figure}

However, because Landlock rules are related to paths, then Landlock performance \textit{should} be influenced by the
number of rules used when specifying multiple paths (folders or files).
In this series of tests, a different number of rules related to multiple folders\footnote{It does not matter if they are
related to a folder or a file, since both are considered as paths.} is taken into account for each performed test.

As can be seen in Figure \ref{fig:perf-test-landlock-impact-folders}, there is a clear correlation
between the number of rules used and the time necessary for Landlock to enforce them.
More specifically, this relation is linear -- this is a reasonable result, since if $n$ rules are given,
then Landlock has to analyse them at least one time in order to enforce all of them.
Hence, an algorithm with linear complexity is the best possible that could be implemented
in this case.

\subsection{eBPF performance scaling}
\label{sec:performance-internal-analysis-ebpf-impact}

Similarly to the previous section, here we will test how eBPF initial policy enforcement scales with the complexity
of a given policy using BPFContain. In order to be able to make a fair comparison with Landlock, only
access to the file system will be taken into consideration, and the tests
will be divided into scaling with the number of permissions on a single folder
and scaling with the number of folders.
Again, the program used for the test will be the one described in Listing \ref{lst:project-perf-program-complex}.

Note that BPFContain sandboxing is ``split into two'' -- the policies are parsed
and implemented by a \textit{server} process, while a \textit{client} process restricts itself
with a chosen policy before running a given program and communicating with the server.
This initial restriction is what will be
measured\footnote{In order to do this, the code of BPFContain has to be modified. The version
used for these tests is available at \url{https://github.com/micheleberetta98/bpfcontain-rs/tree/perf-test}.},
so that a parallel with Landlock can be made.

\begin{figure}[hbt!]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{tests/ebpf-impact.png}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{tests/ebpf-impact-box.png}
  \end{subfigure}

  \caption{Relation between eBPF enforcement and number of permissions.}
  \label{fig:perf-test-ebpf-impact-permissions}
\end{figure}

As before, for the first series of tests permissions will be added one at a time to the policy
which outline is visible in Listing \ref{lst:ebpf-impact-policy-outline}, starting from \texttt{r} (read) and \texttt{w} (write) and ending with
all the permissions allowed by BPFContain\footnote{These are read, write, execute, append, delete, \texttt{chmod},
\texttt{mmap}, linking and \texttt{ioctl}.}.
The results are shown in Figure \ref{fig:perf-test-ebpf-impact-permissions} and, as with Landlock, 
the sandboxing of the running program does not depend by the number of permissions on a single folder.

\vspace*{0.5cm}
\begin{code}[language=yaml, caption=The outline of the policy used for eBPF tests., label=lst:ebpf-impact-policy-outline]
name: test-policy
cmd: ./wasm-runtime program.wasm --no-landlock

allow:
  - dev: terminal
  - fs: {pathname: ./wasm-runtime, access: rxm}
  - fs: {pathname: ./dir, access: rw}
  # Other folders added here
\end{code}

However, when looking at how eBPF scales with the number of paths provided there is a difference
with Landlock. As Figure \ref{fig:perf-test-ebpf-impact-folders} shows, in this case there is no relation between the number
of folders specified and the time taken by BPFContain to containerise the running process.
Here the average time is always around $12.5\ \mu s$, with little variability.

This could be due to the fact that, as already highlighted, BPFContain sandboxing uses two
processes -- a server and a client that communicates with each other.
In this case, the sandboxing done on the client does not depend on how much
the particular policy is complex, since it needs only to know which policy to implement, hence the constant
complexity that emerges from the tests.

\begin{figure}[hbt!]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{tests/ebpf-impact-folders.png}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{tests/ebpf-impact-folders-box.png}
  \end{subfigure}

  \caption{Relation between eBPF enforcement and number of folders.}
  \label{fig:perf-test-ebpf-impact-folders}
\end{figure}

Lastly, note that the times here are all a bit lower than Landlock, but in Section \ref{sec:performance-comparison-between-different-sandboxes}
it was shown that BPFContain was a little slower overall. As hinted before, this could be due to the communication
between the server and the client, so even with more initial overhead Landlock could come out
faster nonetheless.

\clearpage
\section{Conclusions and observations}

One of the key points of this series of tests is that, although perfectly usable,
WebAssembly is not still at the level of performance that can be obtained by using a native binary.
As seen in Section \ref{sec:performance-comparison-between-different-sandboxes}, WASM binaries
always perform worse than the native counterpart, even when not sandboxed.
This result is however to be expected -- a WASM binary is made up of WASM instructions and
it is similar to a sort of \textit{bytecode}, interpreted by the runtime of one's choosing,
and usually a interpreted program\footnote{Despite the program in question being a binary format and hence faster to parse.}
is slower than a compiled one.
This is true also for other languages that follow this paradigm -- for example, Java with its JVM
or Python (that can be compiled into a bytecode before being interpreted) are usually regarded as
slower and/or more resource-hungry when compared to compiled languages such as C or Rust.

When embedding WebAssembly in another language this performance gap can become higher
(although performance itself is still comparable) due to
the work that must be done through the provided interpreting libraries, as has been
highlighted in Section \ref{sec:performance-internal-analysis-execution-times}. 

A positive point that has been shown multiple times is that restricting a binary
with a LSM, be it Landlock or eBPF, does not worsen performance a lot.
As seen in Section \ref{sec:performance-comparison-between-different-sandboxes} a small
performance impact is present, but this could be considered negligible for large programs that take
some time to run. However, for small programs that have an execution time of the same order of magnitude
as the delay introduced this could become problematic if they are called multiple time
and in high frequency.
Furthermore, as found in Section \ref{sec:performance-internal-analysis-landlock-impact}, when using Landlock
as a sandbox the overhead introduced is linear with respect to the number of rules (i.e., paths) specified,
so programs that have to access few locations on the file system have an advantage against programs that need
access to a lot of files and/or directories.
However, this is not the case of eBPF, in which the particular policy only translates into a constant overhead
when restricting the running process, as found in Section \ref{sec:performance-internal-analysis-ebpf-impact}.
The two restriction methods, Landlock and eBPF, are hence both viable, each with its own advantages and downsides
but overall comparable performance.